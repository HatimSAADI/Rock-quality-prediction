# -*- coding: utf-8 -*-
"""FinalNotebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gD9S6qCMBGTXDDi0O8VHjLeCeS8-kXnz
"""

from google.colab import drive
drive.mount('/content/drive/')

"""## **Importing packages**"""

#basic tools 
import os
import numpy as np
import pandas as pd
import warnings
import random


#graph, plots
import matplotlib.pyplot as plt
import seaborn as sns

#building models
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
import lightgbm as lgb
import xgboost as xgb
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
import time
import sys

#metrics 
from sklearn.metrics import roc_auc_score, roc_curve
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## **Importing data and making final train and test**"""

#Import datasets
df_train = pd.read_csv('/content/drive/My Drive/Competition Kaggle/train_1.csv')
df_test = pd.read_csv('/content/drive/My Drive/Competition Kaggle/test_1.csv')

#Delete index xolumns from datasets
df_train.drop(df_train.columns[[0]], axis=1, inplace=True)
df_test.drop(df_test.columns[[0]], axis=1, inplace=True)

#defining numeric columns written as strings
string_train = df_train[['X'	,'Y'	,'Z'	,'PT',	'PP'	,'RP'	,'CO2_B'	,'CAO_B'	,'SIO2_B',	'FE2O3_B'	,'AL2O3_B',	'F_B'	,'CORG_B'	,'NA2O_B'	,'K2O_B'	,'MGO_B'	,'CD_B',	'U_B'	,'AS_B',	'V_B'	,'ZN_B',	'TI_B'	,'TR_B'	,'SR_B',	'CL_B',	'SIO2_L'	,'MGO_L'	,'CD_L',	'CO2_L',	'SO3_B'	,	'CL_L'	,'SIO2R_B'	,'ORDRE','BPL_B']].values
string_test = df_test[['X'	,'Y'	,'Z'	,'PT',	'PP'	,'RP'	,'CO2_B'	,'CAO_B'	,'SIO2_B',	'FE2O3_B'	,'AL2O3_B',	'F_B'	,'CORG_B'	,'NA2O_B'	,'K2O_B'	,'MGO_B'	,'CD_B',	'U_B'	,'AS_B',	'V_B'	,'ZN_B',	'TI_B'	,'TR_B'	,'SR_B',	'CL_B',	'SIO2_L'	,'MGO_L'	,'CD_L',	'CO2_L',	'SO3_B'	,	'CL_L'	,'SIO2R_B'	,'ORDRE']].values

#Transform strings to floats in train and test sets
def correct_point2 (L):
  S1=[]
  for j in L :
    s1 = j.replace(',','.')
    S1.append(s1)
  return S1

def transform_float2 (I) :
  S2=[]
  for i in I :
    f1 = float(i)
    S2.append(f1)
  return S2

#K=np.zeros( (10717, 3) )
def full2(L):
  K=[]
  for i in L :
    k=transform_float2(correct_point2(i))
    K.append(k)
  return K

numeric_train=np.array(full2(string_train))
numeric_test=np.array(full2(string_test))

df_newTr = pd.DataFrame(numeric_train, columns = ['X'	,'Y'	,'Z'	,'PT',	'PP'	,'RP'	,'CO2_B'	,'CAO_B'	,'SIO2_B',	'FE2O3_B'	,'AL2O3_B',	'F_B'	,'CORG_B'	,'NA2O_B'	,'K2O_B'	,'MGO_B'	,'CD_B',	'U_B'	,'AS_B',	'V_B'	,'ZN_B',	'TI_B'	,'TR_B'	,'SR_B',	'CL_B',	'SIO2_L'	,'MGO_L'	,'CD_L',	'CO2_L',	'SO3_B'	,	'CL_L'	,'SIO2R_B'	,'ORDRE', 'BPL_B'])

df_newTe = pd.DataFrame(numeric_test, columns = ['X'	,'Y'	,'Z'	,'PT',	'PP'	,'RP'	,'CO2_B'	,'CAO_B'	,'SIO2_B',	'FE2O3_B'	,'AL2O3_B',	'F_B'	,'CORG_B'	,'NA2O_B'	,'K2O_B'	,'MGO_B'	,'CD_B',	'U_B'	,'AS_B',	'V_B'	,'ZN_B',	'TI_B'	,'TR_B'	,'SR_B',	'CL_B',	'SIO2_L'	,'MGO_L'	,'CD_L',	'CO2_L',	'SO3_B'	,	'CL_L'	,'SIO2R_B'	,'ORDRE'])

Str1 = df_train.drop(columns=['X'	,'Y'	,'Z'	,'PT',	'PP'	,'RP'	,'CO2_B'	,'CAO_B'	,'SIO2_B',	'FE2O3_B'	,'AL2O3_B',	'F_B'	,'CORG_B'	,'NA2O_B'	,'K2O_B'	,'MGO_B'	,'CD_B',	'U_B'	,'AS_B',	'V_B'	,'ZN_B',	'TI_B'	,'TR_B'	,'SR_B',	'CL_B',	'Mo_B'	,'Cr_B',	'SIO2_L'	,'MGO_L'	,'CD_L',	'CO2_L',	'SO3_B'	,'AS_L',	'CL_L'	,'SIO2R_B',	'Th_B'	,'ORDRE', 'BPL_B',	'OBS'	,'RAPPORT_MIN'	,'MINR_PASSANT','CMM', 'CM', 'BPL_B', 'OBJECTID','TRANCHE'])
Str2 = df_test.drop(columns=['X'	,'Y'	,'Z'	,'PT',	'PP'	,'RP'	,'CO2_B'	,'CAO_B'	,'SIO2_B',	'FE2O3_B'	,'AL2O3_B',	'F_B'	,'CORG_B'	,'NA2O_B'	,'K2O_B'	,'MGO_B'	,'CD_B',	'U_B'	,'AS_B',	'V_B'	,'ZN_B',	'TI_B'	,'TR_B'	,'SR_B',	'CL_B',	'Mo_B'	,'Cr_B',	'SIO2_L'	,'MGO_L'	,'CD_L',	'CO2_L',	'SO3_B'	,'AS_L',	'CL_L'	,'SIO2R_B',	'Th_B'	,'ORDRE',	'OBS'	,'RAPPORT_MIN'	,'MINR_PASSANT', 'OBJECTID','CMM', 'CM','TRANCHE'])

new_column1=[]
new_column2=[]
new_column3=[]
for i in df_train["MINR_PASSANT"]:
  if i=='0':
    new_column1.append(1)
    new_column2.append(0)
    new_column3.append(0)
  if i=='10 mm' or i=='10mm':
    new_column1.append(0)
    new_column2.append(0)
    new_column3.append(1)
  if i=='6.3 mm':
    new_column1.append(0)
    new_column2.append(1)
    new_column3.append(0)
  if i=='10 mm et 6.3 mm' or i=='6.3 mm et 10 mm' :
    new_column1.append(0)
    new_column2.append(1)
    new_column3.append(1)

new_column4=[]
new_column5=[]
new_column6=[]
for i in df_test["MINR_PASSANT"]:
  if i=='0':
    new_column4.append(1)
    new_column5.append(0)
    new_column6.append(0)
  if i=='10 mm' or i=='10mm':
    new_column4.append(0)
    new_column5.append(0)
    new_column6.append(1)
  if i=='6.3 mm':
    new_column4.append(0)
    new_column5.append(1)
    new_column6.append(0)
  if i=='10 mm et 6.3 mm' or i=='6.3 mm et 10 mm' :
    new_column4.append(0)
    new_column5.append(1)
    new_column6.append(1)

MINR1 = pd.DataFrame(new_column1, columns=['MINR1'])
MINR2 = pd.DataFrame(new_column2, columns=['MINR2'])
MINR3 = pd.DataFrame(new_column3, columns=['MINR3'])

MINR4 = pd.DataFrame(new_column4, columns=['MINR1'])
MINR5 = pd.DataFrame(new_column5, columns=['MINR2'])
MINR6 = pd.DataFrame(new_column6, columns=['MINR3'])

train = pd.concat([Str1, df_newTr,MINR1, MINR2,MINR3], axis=1)
test = pd.concat([Str2, df_newTe,MINR4, MINR5,MINR6], axis=1)

"""## **Cleaning datasets : missing values and categorical vaiables**"""

#select object columns
obj_col = train.columns[train.dtypes == 'object'].values

#select non object columns
num_col = train.columns[train.dtypes != 'object'].values
num_col_test = test.columns[test.dtypes != 'object'].values

#replace null value in obj columns with None
train[obj_col] = train[obj_col].fillna('None')
test[obj_col] = test[obj_col].fillna('None')

#replace null value in numeric columns with 0
train[num_col] = train[num_col].fillna(0)
test[num_col_test] = test[num_col_test].fillna(0)

train_001 = train
test_001 = test

#Feature to predict
ft_pred = list(set(train_001.columns) - set(test_001.columns))
ft_pred

pip install --upgrade category_encoders

import category_encoders as ce

#Ordinal features
ordinal_features = ['TYPE_'	,'NIVEAU'	,'GISEMENT','ZONE_']


#Split X,y
train_002_X = train_001.drop(ft_pred, axis=1)
train_002_y = train_001[ft_pred]

ce_one_hot = ce.OrdinalEncoder(cols = ordinal_features)

train_003 = pd.concat([ce_one_hot.fit_transform(train_002_X), train_002_y], axis=1, sort=False)
test_003  = ce_one_hot.transform(test_001)

#Nominal features
nominal_features = [x for x in obj_col if x not in ordinal_features]

#Transfer object to int
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()

#for loop nominal feature column
for i in train_003[nominal_features].columns:
    #fit and transform each column and assign to itself
    train_003[i] = labelencoder.fit_transform(train_003[i])
    
#for loop nominal feature column
for i in test_003[nominal_features].columns:
    #fit and transform each column and assign to itself
    test_003[i] = labelencoder.fit_transform(test_003[i])
    
#Get dummy variable for nominal features
train_005 = pd.get_dummies(train_003,columns=nominal_features,drop_first=True)
test_005 = pd.get_dummies(test_003,columns=nominal_features,drop_first=True)

#Only for test set
#Check if any null values
print(train_005.isnull().any().sum())
print(test_005.isnull().any().sum())

#Get missing columns in the training test
missing_cols = set(train_005.drop(columns="BPL_B").columns) - set(test_005.columns)

#Add a missing column in test set with default value equal to 0
for cols in missing_cols:
    test_005[cols] = 0
    
#Ensure the order of column in the test set is in the same order than in train set
test_005 = test_005[train_005.drop(columns="BPL_B").columns]

#Define our variables X and y
X = train_005.drop(columns=["BPL_B"])
y = train_005["BPL_B"]

#correleation matrix
plt.figure(figsize=(50,20))
corrMatrix = X.corr()
sns.heatmap(corrMatrix, annot=True)
plt.show()

"""## **Model 1 : Linear regression**"""

#Split the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

#Training our model
regressor = LinearRegression()
regressor.fit(X_train, y_train)

#Predit and compare
y_pred = regressor.predict(X_test)
comparison_table = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Delta': y_pred-y_test})
print(comparison_table)

#Plot Real vs Predict
plt.figure(figsize=(20,10))
plt.scatter(X_test['X'] * 0.092903, y_test,          color='blue', label='Real',    alpha=0.9, s=20)
plt.scatter(X_test['X'] * 0.092903, y_pred,  color='black' , label='Predict', alpha=0.9, s=20)
plt.title("Real vs Predict")
plt.legend(loc='best')
plt.show()

"""## **Model 2 : Random Forest**"""

#Split the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)

#Let's train our model
regressor = RandomForestRegressor(n_estimators = 100, random_state = 42 )
rf = regressor.fit(X_train, y_train)

#Let's make predictions
predict_train = rf.predict(X_train)
y_pred = rf.predict(X_test)
y_pred

#calculate the rmse to evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("RMSE: %f" % (rmse))

#Plot Real vs Predict
plt.figure(figsize=(20,10))
plt.scatter(X_test['X'] * 0.092903, y_test,          color='blue', label='Real',    alpha=0.9, s=20)
plt.scatter(X_test['X'] * 0.092903, y_pred,  color='red' , label='Predict', alpha=0.9, s=20)
plt.title("Real vs Predict")
plt.legend(loc='best')
plt.show()

#Hyperparameters tuning
from sklearn.model_selection import GridSearchCV

param_grid = [
{'n_estimators': [50, 100, 500], 'max_features': [5, 10], 
 'max_depth': [10, 50, None], 'bootstrap': [True, False]}
]

grid_search_forest = GridSearchCV(rf, param_grid, cv=10, scoring='neg_mean_squared_error')
grid_search_forest.fit(X_train, y_train)

#now let's how the RMSE changes for each parameter configuration
cvres = grid_search_forest.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)

#find the best model of grid search
grid_search_forest.best_estimator_



"""## **Model 3 : XGBOOST Regressor**"""

#split de la data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

#Let's construct our model
modelx = xgb.XGBRegressor(learning_rate =0.1,
 n_estimators=500,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'reg:linear', 
 nthread=4,
 scale_pos_weight=1,
 seed=27)

#Training the model
modelx.fit(X_train,y_train)
preds = modelx.predict(X_test)

#calculate the rmse to evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, preds))
print("RMSE: %f" % (rmse))

#Let's make predictions
y_pred1 = modelx.predict(test_005)
y_pred1

#Plot Real vs Predict
plt.figure(figsize=(20,10))
plt.scatter(X_test['X'] * 0.092903, y_test,          color='red', label='Real',    alpha=0.9, s=20)
plt.scatter(X_test['X'] * 0.092903, preds,  color='blue' , label='Predict', alpha=0.9, s=20)
plt.title("Real vs Predict")
plt.legend(loc='best')
plt.show()

"""## **Model 4 : XGBOOST Regressor with hyperparameters tuning**"""

#hyperparameters fixing
params = {
    # Parameters that we are going to tune.
    'max_depth':6,
    'min_child_weight': 1,
    'eta':.3,
    'subsample': 1,
    'colsample_bytree': 1,
    # Other parameters
    'objective':'reg:linear',
}

#evaluate the model with mae
params['eval_metric'] = "mae"

#train and test matrix
dtrain = xgb.DMatrix(X, label=y)
dtest = xgb.DMatrix(test_005)

num_boost_round = 999

#cross-validation
cv_results = xgb.cv(
    params,
    dtrain,
    num_boost_round=num_boost_round,
    seed=42,
    nfold=5,
    metrics={'mae'},
    early_stopping_rounds=100
)
cv_results

cv_results['test-mae-mean'].min()

#range of values for max_depth and min_child_weight
gridsearch_params = [
    (max_depth, min_child_weight)
    for max_depth in range(9,12)
    for min_child_weight in range(5,8)
]

#searching for optimal values
min_mae = 2.18267
best_params = None
for max_depth, min_child_weight in gridsearch_params:
    print("CV with max_depth={}, min_child_weight={}".format(
                             max_depth,
                             min_child_weight))
    # Update our parameters
    params['max_depth'] = max_depth
    params['min_child_weight'] = min_child_weight
    # Run CV
    cv_results = xgb.cv(
        params,
        dtrain,
        num_boost_round=num_boost_round,
        seed=42,
        nfold=5,
        metrics={'mae'},
        early_stopping_rounds=10
    )
    # Update best MAE
    mean_mae = cv_results['test-mae-mean'].min()
    boost_rounds = cv_results['test-mae-mean'].argmin()
    print("\tMAE {} for {} rounds".format(mean_mae, boost_rounds))
    if mean_mae < min_mae:
        min_mae = mean_mae
        best_params = (max_depth,min_child_weight)
print("Best params: {}, {}, MAE: {}".format(best_params[0], best_params[1], min_mae))

#update the existing values with the best ones
params['max_depth'] = 9
params['min_child_weight'] = 6

#range of values for subsample and colsample
gridsearch_params = [
    (subsample, colsample)
    for subsample in [i/10. for i in range(7,11)]
    for colsample in [i/10. for i in range(7,11)]
]

#searching for optimal values
min_mae = 2.18267
best_params = None
# We start by the largest values and go down to the smallest
for subsample, colsample in reversed(gridsearch_params):
    print("CV with subsample={}, colsample={}".format(
                             subsample,
                             colsample))
    # We update our parameters
    params['subsample'] = subsample
    params['colsample_bytree'] = colsample
    # Run CV
    cv_results = xgb.cv(
        params,
        dtrain,
        num_boost_round=num_boost_round,
        seed=42,
        nfold=5,
        metrics={'mae'},
        early_stopping_rounds=10
    )
    # Update best score
    mean_mae = cv_results['test-mae-mean'].min()
    boost_rounds = cv_results['test-mae-mean'].argmin()
    print("\tMAE {} for {} rounds".format(mean_mae, boost_rounds))
    if mean_mae < min_mae:
        min_mae = mean_mae
        best_params = (subsample,colsample)
print("Best params: {}, {}, MAE: {}".format(best_params[0], best_params[1], min_mae))

#update the existing values with the best ones
params['subsample'] = 1.0
params['colsample_bytree'] = 0.9

# Commented out IPython magic to ensure Python compatibility.
#searching for optimal values
min_mae = 2.18267
best_params = None
for eta in [.3, .2, .1, .05, .01, .005]:
    print("CV with eta={}".format(eta))
    # We update our parameters
    params['eta'] = eta
    # Run and time CV
#     %time cv_results = xgb.cv(params, dtrain, num_boost_round=num_boost_round, seed=42, nfold=5, metrics=['mae'], early_stopping_rounds=1 )
    # Up0date best score
    mean_mae = cv_results['test-mae-mean'].min()
    boost_rounds = cv_results['test-mae-mean'].argmin()
    print("\tMAE {} for {} rounds\n".format(mean_mae, boost_rounds))
    if mean_mae < min_mae:
        min_mae = mean_mae
        best_params = eta
print("Best params: {}, MAE: {}".format(best_params, min_mae))

#update the existing value with the best one
params['eta'] = .01

#Dictionnary of best hyperparameters
params

#Let's train our model
model = xgb.train(
    params,
    dtrain,
    num_boost_round=999,
)

# And use it for predictions.
y_pred = model.predict(dtest)
y_pred

#create submission
submission = pd.DataFrame()
submission['OBJECTID'] = df_test.OBJECTID
submission['BPL_B'] = y_pred

#submission as csv file
submission.to_csv('submission.csv', index=False)